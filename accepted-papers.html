<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Latest Accepted Papers - TGAI</title>
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <nav>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="submission.html">Submission Guidelines</a></li>
      <li><a href="accepted-papers.html">Latest Accepted Papers</a></li>
      <li><a href="editor-board.html">Editor Board</a></li>
      <li><a href="reviewer-guidance.html">Reviewer Guidance</a></li>
    </ul>
  </nav>

  <div class="container">
    <h1>Latest Accepted Papers (January 2026) ðŸ“š</h1>
    
    <div class="paper-card">
      <h2>Paper 1 ðŸŽ¯</h2>
      <p><strong>Title:</strong> Scalable Diffusion Models for High-Resolution Image Generation</p>
      <p><strong>Authors:</strong> John Doe (MIT), Jane Smith (Stanford University)</p>
      <p><strong>Abstract:</strong> This paper presents a novel diffusion model architecture that enables efficient generation of high-resolution images (up to 4K) with reduced computational cost. Our approach achieves state-of-the-art FID scores on multiple benchmarks while maintaining fast inference times.</p>
    </div>

    <div class="paper-card">
      <h2>Paper 2 ðŸš€</h2>
      <p><strong>Title:</strong> Efficient Fine-Tuning Strategies for Large Language Models</p>
      <p><strong>Authors:</strong> Bob Johnson (Harvard University), Alice Lee (UC Berkeley)</p>
      <p><strong>Abstract:</strong> We propose a comprehensive study of parameter-efficient fine-tuning methods for large language models, including LoRA, adapters, and prompt tuning. Our analysis provides practical guidelines for selecting optimal fine-tuning strategies across different downstream tasks and model sizes.</p>
    </div>

    <div class="paper-card">
      <h2>Paper 3 ðŸ’¡</h2>
      <p><strong>Title:</strong> Multimodal Generative AI for Cross-Modal Content Creation</p>
      <p><strong>Authors:</strong> Mike Wang (Johns Hopkins University), Lisa Zhang (Mayo Clinic)</p>
      <p><strong>Abstract:</strong> This work introduces a unified multimodal generative framework that seamlessly integrates text, image, and audio generation. Our model demonstrates superior performance in cross-modal tasks such as text-to-image, image-to-text, and audio-visual synthesis.</p>
    </div>
  </div>
</body>
</html>